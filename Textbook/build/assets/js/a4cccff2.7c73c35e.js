"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[749],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},9071:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"part6/chapter-23-multimodal","title":"Chapter 23: Multi-Modal Interaction","description":"Introduction","source":"@site/docs/part6/chapter-23-multimodal.md","sourceDirName":"part6","slug":"/part6/chapter-23-multimodal","permalink":"/docs/part6/chapter-23-multimodal","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter-23-multimodal.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"sidebar_label":"Chapter 23: Multi-Modal","sidebar_position":23},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 22: Speech & NLU","permalink":"/docs/part6/chapter-22-speech-nlu"}}');var r=i(4848),o=i(8453);const s={sidebar_label:"Chapter 23: Multi-Modal",sidebar_position:23},l="Chapter 23: Multi-Modal Interaction",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Zero-Shot Visual Question Answering (VQA)",id:"zero-shot-visual-question-answering-vqa",level:2},{value:"The Workflow",id:"the-workflow",level:3},{value:"Code Example: VQA with Transformers",id:"code-example-vqa-with-transformers",level:3},{value:"The Grounding Pipeline",id:"the-grounding-pipeline",level:2},{value:"Handling Ambiguity",id:"handling-ambiguity",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-23-multi-modal-interaction",children:"Chapter 23: Multi-Modal Interaction"})}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:['Humans communicate with more than just words. We point, we look, and we refer to objects in our shared environment. "Pick up ',(0,r.jsx)(n.em,{children:"that"}),' cup" is meaningless without visual context. This chapter combines Vision, Language, and Action into a single ',(0,r.jsx)(n.strong,{children:"Multi-Modal"})," pipeline."]}),"\n",(0,r.jsx)(n.h2,{id:"zero-shot-visual-question-answering-vqa",children:"Zero-Shot Visual Question Answering (VQA)"}),"\n",(0,r.jsxs)(n.p,{children:['Traditional computer vision requires training a model on specific classes (e.g., "cup", "bottle"). ',(0,r.jsx)(n.strong,{children:"Zero-Shot"})," models like ",(0,r.jsx)(n.strong,{children:"CLIP"})," (Contrastive Language-Image Pre-training) or ",(0,r.jsx)(n.strong,{children:"LlaVA"})," (Large Language-and-Vision Assistant) can recognize ",(0,r.jsx)(n.em,{children:"any"})," object described in text."]}),"\n",(0,r.jsx)(n.h3,{id:"the-workflow",children:"The Workflow"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input"}),': Image + Question ("Where is the red cup?").']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": Encodes image and text into a shared embedding space."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Bounding box coordinates or a text answer."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"code-example-vqa-with-transformers",children:"Code Example: VQA with Transformers"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from transformers import pipeline\nfrom PIL import Image\nimport requests\n\n# Load Zero-Shot Object Detection pipeline\ndetector = pipeline(model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\")\n\ndef find_object(image, description):\n    predictions = detector(\n        image,\n        candidate_labels=[description],\n    )\n    # predictions = [{'box': {'xmin': 32, 'ymin': 50, ...}, 'score': 0.99, 'label': 'red cup'}]\n    return predictions\n"})}),"\n",(0,r.jsx)(n.h2,{id:"the-grounding-pipeline",children:"The Grounding Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Grounding"}),' is the process of linking a linguistic symbol ("red cup") to a physical entity in the world (ID: 42, Pos: [0.5, 0.2, 0.8]).']}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech"}),': User says "Pick up the red cup."']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Intent Parser"}),": Extracts target object description: ",(0,r.jsx)(n.code,{children:'target="red cup"'}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),': VQA Node searches the current camera frame for "red cup".']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"If object found: Convert 2D bounding box to 3D coordinates using Depth camera."}),"\n",(0,r.jsxs)(n.li,{children:["Send ",(0,r.jsx)(n.code,{children:"Pick(x,y,z)"})," command to the robot arm."]}),"\n",(0,r.jsx)(n.li,{children:'Respond via TTS: "Picking up the red cup."'}),"\n",(0,r.jsx)(n.li,{children:'If not found: Respond: "I don\'t see a red cup."'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,r.jsx)(n.p,{children:'What if the user says "Pick up the cup," but there are two cups? A robust system detects this ambiguity.'}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Count"}),': VQA detects 2 objects matching "cup".']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dialogue Policy"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"If count == 1: Proceed."}),"\n",(0,r.jsx)(n.li,{children:"If count > 1: Ask for clarification."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robot"}),': "I see two cups. Do you mean the left one or the right one?"']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),': The user replies "The left one." The system updates the target filter.']}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"By combining LLMs for logic, VQA for perception, and TTS/STT for communication, we create a system that feels intelligent. The robot isn't just executing code; it's collaborating with you in the physical world."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);