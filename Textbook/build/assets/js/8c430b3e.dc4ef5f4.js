"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[573],{3479(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part4/chapter-14-perception","title":"Chapter 14: AI-Powered Perception and Manipulation","description":"Introduction","source":"@site/docs/part4/chapter-14-perception.md","sourceDirName":"part4","slug":"/part4/chapter-14-perception","permalink":"/docs/part4/chapter-14-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part4/chapter-14-perception.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"sidebar_label":"Chapter 14: AI-powered perception and manipulation","sidebar_position":14},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 13: NVIDIA Isaac SDK and Isaac Sim","permalink":"/docs/part4/chapter-13-isaac-intro"},"next":{"title":"Chapter 15: Reinforcement learning for robot control","permalink":"/docs/part4/chapter-15-rl-control"}}');var i=t(4848),a=t(8453);const s={sidebar_label:"Chapter 14: AI-powered perception and manipulation",sidebar_position:14},o="Chapter 14: AI-Powered Perception and Manipulation",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Setting up a Manipulation Scene",id:"setting-up-a-manipulation-scene",level:2},{value:"Synthetic Data Generation with Replicator",id:"synthetic-data-generation-with-replicator",level:2},{value:"Code Example: <code>synthetic_data_gen.py</code>",id:"code-example-synthetic_data_genpy",level:3},{value:"Annotators and Writers",id:"annotators-and-writers",level:2},{value:"Deep Dive: The Semantic Schema",id:"deep-dive-the-semantic-schema",level:3},{value:"Visualizing Generated Data",id:"visualizing-generated-data",level:2},{value:"Headless vs. UI Generation",id:"headless-vs-ui-generation",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-14-ai-powered-perception-and-manipulation",children:"Chapter 14: AI-Powered Perception and Manipulation"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Traditional computer vision requires gathering thousands of real-world images and painstakingly annotating them by hand. This process is slow, expensive, and error-prone. In this chapter, we tackle the ",(0,i.jsx)(n.strong,{children:"Synthetic Data Gap"})," using Isaac Sim's ",(0,i.jsx)(n.strong,{children:"Replicator"})," engine."]}),"\n",(0,i.jsxs)(n.p,{children:["We will learn to generate ",(0,i.jsx)(n.strong,{children:"Domain Randomized"}),' datasets where lighting, texture, and object pose vary wildly. This prevents your AI model from "overfitting" to specific lighting conditions or background colors, making it robust when deployed in the real world.']}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-a-manipulation-scene",children:"Setting up a Manipulation Scene"}),"\n",(0,i.jsx)(n.p,{children:'Before generating data, we need a 3D scene. We will set up a classic "Tabletop Manipulation" environment.'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Launch Isaac Sim"}),": Use the ",(0,i.jsx)(n.code,{children:"./isaac-sim.sh"})," script or the Launcher."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add Robot"}),": Go to ",(0,i.jsx)(n.code,{children:"Create > Isaac > Robots > Franka Panda"}),". This loads the 7-DOF arm with a gripper."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add Physics"}),": Ensure the robot has an ",(0,i.jsx)(n.code,{children:"ArticulationRoot"})," API applied. This tells the physics engine to treat the hierarchy of links and joints as a single constrained system."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add Objects"}),': Create a simple table and a "target object" (e.g., a cube) for manipulation.',"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Create > Shape > Cube"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Scale it down to 0.05 (5cm)."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Crucial Step"}),": Add ",(0,i.jsx)(n.code,{children:"RigidBody"})," and ",(0,i.jsx)(n.code,{children:"Collision"}),' APIs to the cube via the Property panel. Without these, the cube is just a "ghost" visual geometry that the robot will pass right through.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"synthetic-data-generation-with-replicator",children:"Synthetic Data Generation with Replicator"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Replicator"}),' is Isaac Sim\'s framework for programmatic data generation. It uses a "Graph" based approach: you define randomizers and triggers, and Replicator executes them efficiently at every frame.']}),"\n",(0,i.jsxs)(n.h3,{id:"code-example-synthetic_data_genpy",children:["Code Example: ",(0,i.jsx)(n.code,{children:"synthetic_data_gen.py"})]}),"\n",(0,i.jsx)(n.p,{children:"This script generates RGB images and Semantic Segmentation masks for training a Neural Network."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import omni.replicator.core as rep\r\nimport omni.isaac.core.utils.prims as prim_utils\r\nimport omni.isaac.core.utils.stage as stage_utils\r\nfrom omni.isaac.core.world import World\r\nimport numpy as np\r\n\r\n# 1. Initialize the Simulation World\r\nmy_world = World(stage_units_in_meters=1.0)\r\nmy_world.scene.add_default_ground_plane()\r\n\r\n# 2. Define the Randomization Logic\r\ndef env_props(size=50):\r\n    # Instantiate 'size' copies of the shapes\r\n    instances = rep.randomizer.instantiate(\r\n        rep.utils.get_usd_files(\"props/shapes\"), \r\n        size=size, \r\n        mode='scene_instance'\r\n    )\r\n    \r\n    # Apply Randomizers to these instances\r\n    with instances:\r\n        # Randomize Position within a bounding box\r\n        rep.modify.pose(\r\n            position=rep.distribution.uniform((-1, -1, 0), (1, 1, 0)),\r\n            rotation=rep.distribution.uniform((0,-180, 0), (0, 180, 0)),\r\n        )\r\n        # Randomize Color\r\n        # 'diffuseColor' is the standard USD attribute for base color\r\n        rep.modify.attribute(\"inputs:diffuseColor\", rep.distribution.uniform((0,0,0), (1,1,1)))\r\n        \r\n        # Add Semantic Labels for Segmentation\r\n        # This tags every pixel of these objects with the class 'shape'\r\n        # The Annotator will read this tag to generate the mask.\r\n        rep.modify.semantics([('class', 'shape')])\r\n    return instances\r\n\r\n# 3. Create the Sensor (Camera)\r\ncamera = rep.create.camera(position=(0, 0, 2), look_at=(0,0,0))\r\n# A RenderProduct connects a camera to a resolution. \r\n# Multiple annotators can share the same RenderProduct for efficiency.\r\nrender_product = rep.create.render_product(camera, (1024, 1024))\r\n\r\n# 4. Initialize Writers\r\n# Writers handle saving the raw GPU data to disk formats (PNG, JSON, NumPy)\r\nwriter = rep.WriterRegistry.get(\"BasicWriter\")\r\nwriter.initialize(\r\n    output_dir=\"_output_data\",\r\n    rgb=True,\r\n    semantic_segmentation=True\r\n)\r\nwriter.attach([render_product])\r\n\r\n# 5. Trigger Generation\r\n# Generate 10 frames of data. \r\n# Replicator will step the physics and randomizers automatically.\r\nrep.orchestrator.run_until_frames(10)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"annotators-and-writers",children:"Annotators and Writers"}),"\n",(0,i.jsxs)(n.p,{children:["Replicator separates data ",(0,i.jsx)(n.em,{children:"capture"})," (Annotators) from data ",(0,i.jsx)(n.em,{children:"storage"})," (Writers)."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RGB Annotator"}),": Captures the standard lit color image."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Segmentation Annotator"}),': Renders a "False Color" image. It looks up the ',(0,i.jsx)(n.code,{children:"semanticId"}),' of the object at each pixel. For example, all pixels belonging to "Cube" might be rendered as solid red, while "Background" is black. This provides perfect, pixel-level labels for training segmentation networks (like Mask R-CNN).']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BasicWriter"}),": Saves outputs to disk. For advanced users, you can write custom writers to convert data directly into ",(0,i.jsx)(n.strong,{children:"COCO"})," or ",(0,i.jsx)(n.strong,{children:"YOLO"})," formats, streamlining the pipeline to training tools."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"deep-dive-the-semantic-schema",children:"Deep Dive: The Semantic Schema"}),"\n",(0,i.jsx)(n.p,{children:"USD semantics are powerful. You can have multiple types of labels on the same object."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"class"}),': General category (e.g., "car", "pedestrian").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"color"}),': Specific attribute (e.g., "red", "blue").\r\nReplicator allows you to filter which tag type the annotator should listen to. By default, it uses ',(0,i.jsx)(n.code,{children:"class"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"visualizing-generated-data",children:"Visualizing Generated Data"}),"\n",(0,i.jsxs)(n.p,{children:["After running the script, navigate to the ",(0,i.jsx)(n.code,{children:"_output_data"})," folder. You will see pairs of images:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"rgb_0001.png"}),": The photorealistic scene with random object positions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"semantic_segmentation_0001.png"}),": The segmentation mask."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Pro Tip"}),": Use the ",(0,i.jsx)(n.code,{children:"Colorize"})," setting in the writer or a python script to map the integer segmentation IDs to visible colors for easier debugging. By default, the IDs might be close to 0 (e.g., 1, 2, 3), making the image look pitch black to the naked eye unless normalized."]}),"\n",(0,i.jsx)(n.h2,{id:"headless-vs-ui-generation",children:"Headless vs. UI Generation"}),"\n",(0,i.jsx)(n.p,{children:"While we ran this interactively, for large datasets (100k+ images), you should run Headless."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./isaac-sim.sh --no-window --exec synthetic_data_gen.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"This frees up GPU VRAM from rendering the UI, allowing you to spawn more assets or run parallel instances."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);