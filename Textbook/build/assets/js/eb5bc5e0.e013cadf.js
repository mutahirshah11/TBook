"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[356],{5431:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"part4/chapter-15-rl-control","title":"Chapter 15: Reinforcement Learning for Robot Control","description":"Introduction","source":"@site/docs/part4/chapter-15-rl-control.md","sourceDirName":"part4","slug":"/part4/chapter-15-rl-control","permalink":"/Tbook/docs/part4/chapter-15-rl-control","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part4/chapter-15-rl-control.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"sidebar_label":"Chapter 15: Reinforcement learning for robot control","sidebar_position":15},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 14: AI-powered perception and manipulation","permalink":"/Tbook/docs/part4/chapter-14-perception"},"next":{"title":"Chapter 16: Sim-to-real transfer techniques","permalink":"/Tbook/docs/part4/chapter-16-sim-to-real"}}');var o=r(4848),t=r(8453);const s={sidebar_label:"Chapter 15: Reinforcement learning for robot control",sidebar_position:15},a="Chapter 15: Reinforcement Learning for Robot Control",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"RL Concepts in Isaac Lab",id:"rl-concepts-in-isaac-lab",level:2},{value:"The Manager-Based Environment",id:"the-manager-based-environment",level:3},{value:"Training Workflow",id:"training-workflow",level:2},{value:"1. Configuration: The Config Class",id:"1-configuration-the-config-class",level:3},{value:"2. Reward Shaping: The Art of RL",id:"2-reward-shaping-the-art-of-rl",level:3},{value:"3. Curriculum Learning",id:"3-curriculum-learning",level:3},{value:"4. Running Headless",id:"4-running-headless",level:3},{value:"5. Monitoring with Tensorboard",id:"5-monitoring-with-tensorboard",level:3},{value:"Running Inference",id:"running-inference",level:2},{value:"Common Pitfalls",id:"common-pitfalls",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-15-reinforcement-learning-for-robot-control",children:"Chapter 15: Reinforcement Learning for Robot Control"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["Reinforcement Learning (RL) has solved some of the hardest problems in robotics, from quadruped locomotion to in-hand manipulation. This chapter covers training a robot control policy using ",(0,o.jsx)(n.strong,{children:"Isaac Lab"}),", NVIDIA's unified framework for robot learning. Isaac Lab provides high-performance, GPU-accelerated environments that run thousands of times faster than real-time."]}),"\n",(0,o.jsx)(n.h2,{id:"rl-concepts-in-isaac-lab",children:"RL Concepts in Isaac Lab"}),"\n",(0,o.jsxs)(n.p,{children:["Isaac Lab uses ",(0,o.jsx)(n.strong,{children:"PPO (Proximal Policy Optimization)"})," as its default algorithm. It is a policy-gradient method known for its stability and ease of tuning."]}),"\n",(0,o.jsx)(n.h3,{id:"the-manager-based-environment",children:"The Manager-Based Environment"}),"\n",(0,o.jsxs)(n.p,{children:["Unlike traditional OpenAI Gym environments where logic is monolithic, Isaac Lab uses a ",(0,o.jsx)(n.strong,{children:"Manager-Based"})," architecture. Logic is decoupled into managers:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scene Manager"}),": Handles spawning robots and objects via USD."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Event Manager"}),": Handles randomization (resets, mass changes)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Observation Manager"}),": Computes inputs for the neural net."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reward Manager"}),": Computes the score for the current step."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'This modularity allows you to mix-and-match. For example, you can reuse the "Velocity Tracking Reward" across different robots (Ant, Humanoid, Quadruped) without rewriting code.'}),"\n",(0,o.jsx)(n.h2,{id:"training-workflow",children:"Training Workflow"}),"\n",(0,o.jsx)(n.h3,{id:"1-configuration-the-config-class",children:"1. Configuration: The Config Class"}),"\n",(0,o.jsxs)(n.p,{children:["Environments are defined using Python ",(0,o.jsx)(n.code,{children:"dataclasses"}),". This provides type safety and auto-completion, a major upgrade over untyped YAML files."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from isaaclab.envs import DirectRLEnvCfg\r\nfrom isaaclab.scene import InteractiveSceneCfg\r\nfrom isaaclab.sim import SimulationCfg\r\nfrom isaaclab.assets import ArticulationCfg\r\n\r\n@configclass\r\nclass MyRobotEnvCfg(DirectRLEnvCfg):\r\n    # 1. Simulation Physics (Dt, Gravity, Device)\r\n    sim: SimulationCfg = SimulationCfg(dt=0.005, gravity=(0.0, 0.0, -9.81))\r\n    \r\n    # 2. The Robot Asset (The USD file to load)\r\n    robot: ArticulationCfg = ArticulationCfg(\r\n        prim_path="/World/Robot", \r\n        spawn=UsdFileCfg(usd_path="path/to/robot.usd"),\r\n        init_state=ArticulationCfg.InitialStateCfg(pos=(0.0, 0.0, 0.5))\r\n    )\r\n    \r\n    # 3. Actions: What can the policy control?\r\n    # Here we control joint efforts (torques)\r\n    actions: ActionCfg = ActionCfg(asset_name="robot", joint_names=[".*"], scale=1.0)\r\n\r\n    # 4. Observations: What does the policy see?\r\n    # Usually includes Joint Positions, Velocities, and Commands\r\n    observations: ObservationCfg = ObservationCfg(...)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-reward-shaping-the-art-of-rl",children:"2. Reward Shaping: The Art of RL"}),"\n",(0,o.jsxs)(n.p,{children:["The reward function is the most critical part of your design. It tells the robot ",(0,o.jsx)(n.em,{children:"what"})," to do, not ",(0,o.jsx)(n.em,{children:"how"})," to do it."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dense Rewards"}),': Continuous feedback (e.g., "distance to target"). Good for learning fast.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sparse Rewards"}),': Binary feedback (e.g., "goal reached" +1, else 0). Hard to learn but often results in more natural motion.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Penalties"}),': Negative rewards (e.g., "energy consumption", "impact force"). Crucial for Sim-to-Real safety to prevent the robot from flailing wildly.']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-curriculum-learning",children:"3. Curriculum Learning"}),"\n",(0,o.jsxs)(n.p,{children:["For complex tasks like walking, starting with a difficult goal leads to failure. Isaac Lab supports ",(0,o.jsx)(n.strong,{children:"Curriculum Learning"}),". You can define terms in your config that scale difficulty based on success.\r\n",(0,o.jsx)(n.em,{children:"Example"}),": Start with a command velocity of 0.0 m/s (standing still). If the robot survives 100 steps, increase the target to 0.1 m/s, then 0.2 m/s, and so on."]}),"\n",(0,o.jsx)(n.h3,{id:"4-running-headless",children:"4. Running Headless"}),"\n",(0,o.jsx)(n.p,{children:"Training requires millions of data points. Rendering graphics consumes GPU resources needed for physics and neural network updates."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Headless = No Window = Maximum Speed\r\n# We use the provided script which sets up the python path correctly\r\n./isaaclab.sh -p source/standalone/workflows/rsl_rl/train.py task=Isaac-Ant-v0 --headless\n"})}),"\n",(0,o.jsx)(n.h3,{id:"5-monitoring-with-tensorboard",children:"5. Monitoring with Tensorboard"}),"\n",(0,o.jsxs)(n.p,{children:["RL is notoriously unstable. You must monitor the ",(0,o.jsx)(n.code,{children:"reward"})," curves."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"tensorboard --logdir logs/rsl_rl/ant\n"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Rising Reward"}),": Good. The robot is learning."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Flat Reward"}),": Bad. The task might be too hard, or hyperparameters (learning rate) need tuning."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Episode Length"}),": Should increase over time (robot isn't falling over immediately)."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"running-inference",children:"Running Inference"}),"\n",(0,o.jsxs)(n.p,{children:["Once training is complete, we run ",(0,o.jsx)(n.strong,{children:"Inference"}),". This loads the trained neural network weights (",(0,o.jsx)(n.code,{children:".pt"})," file) and runs the simulation ",(0,o.jsx)(n.strong,{children:"with rendering enabled"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py task=Isaac-Ant-v0 num_envs=16\n"})}),"\n",(0,o.jsx)(n.p,{children:"You will see 16 robots executing your policy. Note that inference is deterministic (randomness is usually turned off), allowing you to strictly evaluate performance."}),"\n",(0,o.jsx)(n.h3,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Exploding Gradients"}),": If the physics simulation is unstable (NaNs), the policy weights will explode. Check your collision meshes and P-Gains."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reward Hacking"}),": The robot finds a loophole (e.g., falling over forward to maximize velocity for one frame). You need to add termination conditions to prevent this."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);