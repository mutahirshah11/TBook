"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[409],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}},8636:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"part5/chapter-20-hri","title":"Chapter 20: Natural Human-Robot Interaction Design","description":"Introduction","source":"@site/docs/part5/chapter-20-hri.md","sourceDirName":"part5","slug":"/part5/chapter-20-hri","permalink":"/Tbook/docs/part5/chapter-20-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part5/chapter-20-hri.md","tags":[],"version":"current","sidebarPosition":20,"frontMatter":{"sidebar_label":"Chapter 20: HRI","sidebar_position":20},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 19: Manipulation","permalink":"/Tbook/docs/part5/chapter-19-manipulation"},"next":{"title":"Chapter 21: LLM Integration","permalink":"/Tbook/docs/part6/chapter-21-llm-integration"}}');var i=t(4848),o=t(8453);const s={sidebar_label:"Chapter 20: HRI",sidebar_position:20},a="Chapter 20: Natural Human-Robot Interaction Design",d={},l=[{value:"Introduction",id:"introduction",level:2},{value:"The Interaction Loop",id:"the-interaction-loop",level:2},{value:"Keyword Spotting (KWS)",id:"keyword-spotting-kws",level:2},{value:"Code Example: <code>voice_command.py</code>",id:"code-example-voice_commandpy",level:3},{value:"Gesture Recognition with MediaPipe",id:"gesture-recognition-with-mediapipe",level:2},{value:"Vector Math for Gestures",id:"vector-math-for-gestures",level:3},{value:"Code Example: <code>gesture_detect.py</code>",id:"code-example-gesture_detectpy",level:3},{value:"Integration: The &quot;Look-and-Point&quot; System",id:"integration-the-look-and-point-system",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-20-natural-human-robot-interaction-design",children:"Chapter 20: Natural Human-Robot Interaction Design"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["A humanoid robot that requires a keyboard to operate is just a machine. A humanoid that responds to a wave and a voice command feels like a partner. This chapter integrates ",(0,i.jsx)(n.strong,{children:"Keyword Spotting"})," and ",(0,i.jsx)(n.strong,{children:"Gesture Recognition"})," to build a natural, multimodal interface for the Talos robot."]}),"\n",(0,i.jsx)(n.h2,{id:"the-interaction-loop",children:"The Interaction Loop"}),"\n",(0,i.jsxs)(n.p,{children:["We will build a ",(0,i.jsx)(n.strong,{children:"Finite State Machine (FSM)"})," to manage the interaction."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Idle"}),": Robot scans for a person."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Listening"}),': Triggered by a "Wave" gesture. Robot looks at user.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Active"}),': Processing voice command (e.g., "Grasp that").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Executing"}),": Robot performs the task."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"keyword-spotting-kws",children:"Keyword Spotting (KWS)"}),"\n",(0,i.jsxs)(n.p,{children:["We don't need a full Large Language Model (LLM) running locally to understand basic commands. ",(0,i.jsx)(n.strong,{children:"Keyword Spotting"})," is efficient and low-latency. We use ",(0,i.jsx)(n.strong,{children:"Vosk"}),", which runs offline and fits on embedded CPUs."]}),"\n",(0,i.jsxs)(n.h3,{id:"code-example-voice_commandpy",children:["Code Example: ",(0,i.jsx)(n.code,{children:"voice_command.py"})]}),"\n",(0,i.jsx)(n.p,{children:"This script uses a grammar constraint to only listen for specific words, vastly increasing accuracy."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from vosk import Model, KaldiRecognizer\nimport pyaudio\nimport json\n\n# 1. Load Model\n# Download \'vosk-model-small-en-us-0.15\' for speed\nmodel = Model("model")\n\n# 2. Constrain Grammar\n# Only listen for these specific JSON tokens\ngrammar = \'["robot", "stop", "go", "grasp", "left", "right", "[unk]"]\'\nrec = KaldiRecognizer(model, 16000, grammar)\n\np = pyaudio.PyAudio()\nstream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8000)\n\nprint("Listening...")\n\nwhile True:\n    data = stream.read(4000, exception_on_overflow=False)\n    if rec.AcceptWaveform(data):\n        result = json.loads(rec.Result())\n        text = result[\'text\']\n        \n        if "robot grasp" in text:\n            print("CMD: GRASP_INITIATED")\n            # trigger_grasp_fsm()\n        elif "robot stop" in text:\n            print("CMD: E-STOP")\n            # trigger_estop()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"gesture-recognition-with-mediapipe",children:"Gesture Recognition with MediaPipe"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Google MediaPipe"}),' offers real-time hand tracking. Instead of just detecting a "hand", we will compute ',(0,i.jsx)(n.strong,{children:"geometric features"})," to recognize gestures."]}),"\n",(0,i.jsx)(n.h3,{id:"vector-math-for-gestures",children:"Vector Math for Gestures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:'"Stop" Gesture'}),": All 5 fingers extended, palm facing camera.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Check ",(0,i.jsx)(n.code,{children:"dist(wrist, fingertip)"})," is large for all fingers."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:'"Point" Gesture'}),": Index finger extended, others curled.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The vector ",(0,i.jsx)(n.code,{children:"v = (index_tip - index_mcp)"})," defines the pointing direction in 3D space."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"code-example-gesture_detectpy",children:["Code Example: ",(0,i.jsx)(n.code,{children:"gesture_detect.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import mediapipe as mp\nimport cv2\nimport numpy as np\n\nmp_hands = mp.solutions.hands\nhands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n\ndef get_pointing_vector(landmarks):\n    # Extract 3D coordinates\n    index_tip = np.array([landmarks[8].x, landmarks[8].y, landmarks[8].z])\n    index_mcp = np.array([landmarks[5].x, landmarks[5].y, landmarks[5].z])\n    \n    # Vector from knuckle to tip\n    vector = index_tip - index_mcp\n    return vector / np.linalg.norm(vector)\n\ndef main():\n    cap = cv2.VideoCapture(0)\n    while cap.isOpened():\n        success, image = cap.read()\n        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        \n        if results.multi_hand_landmarks:\n            for hand_lms in results.multi_hand_landmarks:\n                # Logic: Is Index extended? Are others curled?\n                # ... (Geometric checks) ...\n                \n                vec = get_pointing_vector(hand_lms.landmark)\n                print(f\"User is pointing: {vec}\")\n                \n                # Visualization\n                mp.solutions.drawing_utils.draw_landmarks(image, hand_lms, mp_hands.HAND_CONNECTIONS)\n                \n        cv2.imshow('HRI View', image)\n        if cv2.waitKey(5) & 0xFF == 27: break\n\n    cap.release()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-the-look-and-point-system",children:'Integration: The "Look-and-Point" System'}),"\n",(0,i.jsx)(n.p,{children:'By combining these, we solve the "Grounding Problem" (what does "that" mean?).'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice"}),': User says "Look at ',(0,i.jsx)(n.em,{children:"that"}),'".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gesture"}),": User points."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fusion"}),": The robot intersects the ",(0,i.jsx)(n.strong,{children:"Pointing Ray"})," (from MediaPipe) with the ",(0,i.jsx)(n.strong,{children:"World Map"}),' (Octomap/Collision map). The first object hit by the ray is the target "that".']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:'HRI is about establishing a shared reality. By implementing robust keyword spotting and geometric gesture parsing, we allow the user to guide the robot\'s high-level autonomy ("Go there") without needing to joystick it manually ("Move forward 5 meters").'})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);