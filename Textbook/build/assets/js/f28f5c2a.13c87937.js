"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[342],{6791:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part6/chapter-22-speech-nlu","title":"Chapter 22: Speech Recognition and Natural Language Understanding","description":"Introduction","source":"@site/docs/part6/chapter-22-speech-nlu.md","sourceDirName":"part6","slug":"/part6/chapter-22-speech-nlu","permalink":"/Tbook/docs/part6/chapter-22-speech-nlu","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter-22-speech-nlu.md","tags":[],"version":"current","sidebarPosition":22,"frontMatter":{"sidebar_label":"Chapter 22: Speech & NLU","sidebar_position":22},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 21: LLM Integration","permalink":"/Tbook/docs/part6/chapter-21-llm-integration"},"next":{"title":"Chapter 23: Multi-Modal","permalink":"/Tbook/docs/part6/chapter-23-multimodal"}}');var o=t(4848),s=t(8453);const i={sidebar_label:"Chapter 22: Speech & NLU",sidebar_position:22},a="Chapter 22: Speech Recognition and Natural Language Understanding",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Offline Speech-to-Text with Vosk",id:"offline-speech-to-text-with-vosk",level:2},{value:"Code Example: Speech Publisher Node",id:"code-example-speech-publisher-node",level:3},{value:"Intent Classification with Grammars",id:"intent-classification-with-grammars",level:2},{value:"Text-to-Speech with Piper",id:"text-to-speech-with-piper",level:2},{value:"Integration",id:"integration",level:3},{value:"Closing the Loop",id:"closing-the-loop",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-22-speech-recognition-and-natural-language-understanding",children:"Chapter 22: Speech Recognition and Natural Language Understanding"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Voice is the most natural interface for humans. In this chapter, we give the robot ears (Speech-to-Text) and a voice (Text-to-Speech), creating a full verbal interaction loop."}),"\n",(0,o.jsx)(n.h2,{id:"offline-speech-to-text-with-vosk",children:"Offline Speech-to-Text with Vosk"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vosk"})," is an offline speech recognition toolkit capable of running on embedded devices like the Raspberry Pi 4. It is robust to noise and doesn't require an API key."]}),"\n",(0,o.jsx)(n.h3,{id:"code-example-speech-publisher-node",children:"Code Example: Speech Publisher Node"}),"\n",(0,o.jsx)(n.p,{children:"This node listens to the microphone and publishes the text only when a sentence is complete."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vosk import Model, KaldiRecognizer\r\nimport pyaudio\r\nimport json\r\n\r\nclass SpeechNode(Node):\r\n    def __init__(self):\r\n        super().__init__('speech_node')\r\n        self.pub = self.create_publisher(String, '/audio/speech_text', 10)\r\n        \r\n        model = Model(\"model\") # Path to unzipped Vosk model\r\n        self.rec = KaldiRecognizer(model, 16000)\r\n        \r\n        self.p = pyaudio.PyAudio()\r\n        self.stream = self.p.open(\r\n            format=pyaudio.paInt16, \r\n            channels=1, \r\n            rate=16000, \r\n            input=True, \r\n            frames_per_buffer=8000)\r\n            \r\n        self.timer = self.create_timer(0.1, self.listen_loop)\r\n\r\n    def listen_loop(self):\r\n        data = self.stream.read(4000, exception_on_overflow=False)\r\n        if self.rec.AcceptWaveform(data):\r\n            result = json.loads(self.rec.Result())\r\n            text = result['text']\r\n            if text:\r\n                msg = String()\r\n                msg.data = text\r\n                self.pub.publish(msg)\r\n                self.get_logger().info(f\"Heard: {text}\")\r\n\r\ndef main():\r\n    # ... standard boilerplate ...\n"})}),"\n",(0,o.jsx)(n.h2,{id:"intent-classification-with-grammars",children:"Intent Classification with Grammars"}),"\n",(0,o.jsxs)(n.p,{children:['NLU (Natural Language Understanding) converts raw text ("Go to the kitchen") into structured intent (',(0,o.jsx)(n.code,{children:"NAVIGATE"}),", ",(0,o.jsx)(n.code,{children:"KITCHEN"}),")."]}),"\n",(0,o.jsxs)(n.p,{children:["Vosk supports ",(0,o.jsx)(n.strong,{children:"Grammar Constraints"}),'. By providing a list of allowed words, you drastically increase accuracy and reduce "hallucinations" (hearing words that weren\'t said).']}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Only listen for these words\r\ngrammar = \'["robot", "stop", "go", "kitchen", "living room"]\'\r\nrec = KaldiRecognizer(model, 16000, grammar)\n'})}),"\n",(0,o.jsx)(n.p,{children:'Now, if someone says "Um, robot, please go to the kitchen now," Vosk will robustly extract "robot go kitchen".'}),"\n",(0,o.jsx)(n.h2,{id:"text-to-speech-with-piper",children:"Text-to-Speech with Piper"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Piper"})," is a fast, local neural text-to-speech system. It sounds nearly human and runs faster than real-time even on low-end hardware."]}),"\n",(0,o.jsx)(n.h3,{id:"integration",children:"Integration"}),"\n",(0,o.jsxs)(n.p,{children:["We create a Subscriber Node that listens to ",(0,o.jsx)(n.code,{children:"/audio/speak"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import subprocess\r\n\r\ndef speak_callback(self, msg):\r\n    text = msg.data\r\n    # Pipe text to the piper executable\r\n    cmd = f\"echo '{text}' | piper --model en_US-lessac-medium.onnx --output_raw | aplay -r 22050 -f S16_LE -t raw -\"\r\n    subprocess.Popen(cmd, shell=True)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"closing-the-loop",children:"Closing the Loop"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vosk Node"}),' publishes: "Robot, tell me a joke."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Brain Node"})," (Chapter 21) receives text, calls LLM Action."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM"}),' returns: "Why did the robot cross the road? To optimize its path."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Brain Node"})," publishes response to ",(0,o.jsx)(n.code,{children:"/audio/speak"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Piper Node"})," reads text and plays audio."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var r=t(6540);const o={},s=r.createContext(o);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);