"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[181],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var r=t(6540);const o={},a=r.createContext(o);function s(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:n},e.children)}},9956:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part6/chapter-21-llm-integration","title":"Chapter 21: Integrating GPT Models for Conversational AI","description":"Introduction","source":"@site/docs/part6/chapter-21-llm-integration.md","sourceDirName":"part6","slug":"/part6/chapter-21-llm-integration","permalink":"/Tbook/docs/part6/chapter-21-llm-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter-21-llm-integration.md","tags":[],"version":"current","sidebarPosition":21,"frontMatter":{"sidebar_label":"Chapter 21: LLM Integration","sidebar_position":21},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 20: HRI","permalink":"/Tbook/docs/part5/chapter-20-hri"},"next":{"title":"Chapter 22: Speech & NLU","permalink":"/Tbook/docs/part6/chapter-22-speech-nlu"}}');var o=t(4848),a=t(8453);const s={sidebar_label:"Chapter 21: LLM Integration",sidebar_position:21},l="Chapter 21: Integrating GPT Models for Conversational AI",i={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Cloud vs. Local LLMs",id:"cloud-vs-local-llms",level:2},{value:"Cloud APIs (e.g., OpenAI GPT-4)",id:"cloud-apis-eg-openai-gpt-4",level:3},{value:"Local LLMs (e.g., Ollama / Llama 3)",id:"local-llms-eg-ollama--llama-3",level:3},{value:"The ROS 2 Action Server Pattern",id:"the-ros-2-action-server-pattern",level:2},{value:"Tutorial: Defining the GenerateText Action",id:"tutorial-defining-the-generatetext-action",level:2},{value:"Code Example: Ollama Action Server",id:"code-example-ollama-action-server",level:2},{value:"Prompt Engineering for Robots",id:"prompt-engineering-for-robots",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-21-integrating-gpt-models-for-conversational-ai",children:"Chapter 21: Integrating GPT Models for Conversational AI"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:'Large Language Models (LLMs) like GPT-4 and Llama 3 have revolutionized natural language understanding. In robotics, they act as the "cognitive engine," allowing robots to reason about tasks, plan actions, and converse with humans. This chapter covers how to integrate these models into a ROS 2 system.'}),"\n",(0,o.jsx)(n.h2,{id:"cloud-vs-local-llms",children:"Cloud vs. Local LLMs"}),"\n",(0,o.jsx)(n.h3,{id:"cloud-apis-eg-openai-gpt-4",children:"Cloud APIs (e.g., OpenAI GPT-4)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pros"}),": Highest intelligence, easy to use API, no local hardware required."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cons"}),": Latency (network + inference), privacy concerns, cost."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"local-llms-eg-ollama--llama-3",children:"Local LLMs (e.g., Ollama / Llama 3)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pros"}),": Privacy (data never leaves the robot), consistent latency, free."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cons"}),": Requires powerful hardware (GPU/RAM), slightly lower reasoning capability compared to massive cloud models."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["For autonomous robots, ",(0,o.jsx)(n.strong,{children:"Local LLMs"})," are often preferred to ensure operation without internet access. We will use ",(0,o.jsx)(n.strong,{children:"Ollama"}),", a lightweight framework for running Llama 3, Mistral, and other open models."]}),"\n",(0,o.jsx)(n.h2,{id:"the-ros-2-action-server-pattern",children:"The ROS 2 Action Server Pattern"}),"\n",(0,o.jsxs)(n.p,{children:["LLM generation takes time\u2014from milliseconds to seconds. Using a standard ROS 2 Service (",(0,o.jsx)(n.code,{children:"srv"}),") blocks the client until generation is complete, which can freeze your robot's behavior loop."]}),"\n",(0,o.jsxs)(n.p,{children:["Instead, we use a ",(0,o.jsx)(n.strong,{children:"ROS 2 Action"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Goal"}),': The prompt ("Plan a path to the kitchen").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback"}),': Stream of tokens as they are generated ("To...", "get...", "to...", "the...").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Result"}),": The final complete response."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'This allows the robot to "think out loud" or cancel the thought process if the situation changes.'}),"\n",(0,o.jsx)(n.h2,{id:"tutorial-defining-the-generatetext-action",children:"Tutorial: Defining the GenerateText Action"}),"\n",(0,o.jsxs)(n.p,{children:["Create a new file ",(0,o.jsx)(n.code,{children:"action/GenerateText.action"})," in your package:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:'# Goal\nstring prompt\nstring system_prompt "You are a helpful robot assistant."\nfloat32 temperature 0.7\n---\n# Result\nstring response\nbool success\n---\n# Feedback\nstring partial_response\n'})}),"\n",(0,o.jsx)(n.h2,{id:"code-example-ollama-action-server",children:"Code Example: Ollama Action Server"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom robot_interfaces.action import GenerateText\nimport ollama\n\nclass OllamaActionServer(Node):\n    def __init__(self):\n        super().__init__('ollama_server')\n        self._action_server = ActionServer(\n            self,\n            GenerateText,\n            'generate_text',\n            self.execute_callback)\n        self.get_logger().info(\"Ollama Action Server Ready\")\n\n    async def execute_callback(self, goal_handle):\n        self.get_logger().info(f\"Generating for: {goal_handle.request.prompt}\")\n        feedback_msg = GenerateText.Feedback()\n        \n        # Stream response from Ollama\n        full_response = \"\"\n        stream = ollama.chat(\n            model='llama3',\n            messages=[\n                {'role': 'system', 'content': goal_handle.request.system_prompt},\n                {'role': 'user', 'content': goal_handle.request.prompt}\n            ],\n            stream=True,\n        )\n\n        for chunk in stream:\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return GenerateText.Result(success=False, response=full_response)\n            \n            token = chunk['message']['content']\n            full_response += token\n            feedback_msg.partial_response = full_response\n            goal_handle.publish_feedback(feedback_msg)\n\n        goal_handle.succeed()\n        return GenerateText.Result(success=True, response=full_response)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OllamaActionServer()\n    rclpy.spin(node)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"prompt-engineering-for-robots",children:"Prompt Engineering for Robots"}),"\n",(0,o.jsxs)(n.p,{children:["Robots need structured outputs (e.g., JSON coordinates), not poetry. Use the ",(0,o.jsx)(n.strong,{children:"System Prompt"})," to enforce constraints:"]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"\"You are a navigation agent. Output ONLY a JSON object with keys 'target' and 'action'. Do not include markdown formatting.\""}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"By constraining the output, you can parse the LLM's response directly into robot commands."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);