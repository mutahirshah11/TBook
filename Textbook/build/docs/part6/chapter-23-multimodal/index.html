<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-part6/chapter-23-multimodal" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 23: Multi-Modal Interaction | Robotics Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://mutahirshah11.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://mutahirshah11.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://mutahirshah11.github.io/docs/part6/chapter-23-multimodal"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 23: Multi-Modal Interaction | Robotics Book"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mutahirshah11.github.io/docs/part6/chapter-23-multimodal"><link data-rh="true" rel="alternate" href="https://mutahirshah11.github.io/docs/part6/chapter-23-multimodal" hreflang="en"><link data-rh="true" rel="alternate" href="https://mutahirshah11.github.io/docs/part6/chapter-23-multimodal" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 23: Multi-Modal","item":"https://mutahirshah11.github.io/docs/part6/chapter-23-multimodal"}]}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&amp;display=swap">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;display=swap"><link rel="stylesheet" href="/assets/css/styles.e995a001.css">
<script src="/assets/js/runtime~main.afddf8a6.js" defer="defer"></script>
<script src="/assets/js/main.114982c8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","dark"),document.documentElement.setAttribute("data-theme-choice","dark"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="RoboLearn Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="RoboLearn Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">RoboLearn</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/part1/foundations-physical-ai">Curriculum</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link navbar-login-link" href="/signin">Sign In</a><a class="navbar__item navbar__link button button--primary navbar-cta" href="/signup">Sign Up</a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/part1/foundations-physical-ai"><span title="Part 1: Foundations of Physical AI" class="categoryLinkLabel_W154">Part 1: Foundations of Physical AI</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/part2/chapter-5-ros2-architecture-and-core-concepts"><span title="Part 2: Embodied AI &amp; ROS 2 Foundations" class="categoryLinkLabel_W154">Part 2: Embodied AI &amp; ROS 2 Foundations</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/part3/chapter_9"><span title="Part 3: Robot Simulation with Gazebo" class="categoryLinkLabel_W154">Part 3: Robot Simulation with Gazebo</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/part4/chapter-13-isaac-intro"><span title="Part 4: NVIDIA Isaac Platform" class="categoryLinkLabel_W154">Part 4: NVIDIA Isaac Platform</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/part5/chapter-17-kinematics-dynamics"><span title="Part 5: Humanoid Robot Development" class="categoryLinkLabel_W154">Part 5: Humanoid Robot Development</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/part6/chapter-21-llm-integration"><span title="Part 6: Conversational Robotics" class="categoryLinkLabel_W154">Part 6: Conversational Robotics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/part6/chapter-21-llm-integration"><span title="Chapter 21: LLM Integration" class="linkLabel_WmDU">Chapter 21: LLM Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/part6/chapter-22-speech-nlu"><span title="Chapter 22: Speech &amp; NLU" class="linkLabel_WmDU">Chapter 22: Speech &amp; NLU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/part6/chapter-23-multimodal"><span title="Chapter 23: Multi-Modal" class="linkLabel_WmDU">Chapter 23: Multi-Modal</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Part 6: Conversational Robotics</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 23: Multi-Modal</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 23: Multi-Modal Interaction</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>Humans communicate with more than just words. We point, we look, and we refer to objects in our shared environment. &quot;Pick up <em>that</em> cup&quot; is meaningless without visual context. This chapter combines Vision, Language, and Action into a single <strong>Multi-Modal</strong> pipeline.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="zero-shot-visual-question-answering-vqa">Zero-Shot Visual Question Answering (VQA)<a href="#zero-shot-visual-question-answering-vqa" class="hash-link" aria-label="Direct link to Zero-Shot Visual Question Answering (VQA)" title="Direct link to Zero-Shot Visual Question Answering (VQA)" translate="no">​</a></h2>
<p>Traditional computer vision requires training a model on specific classes (e.g., &quot;cup&quot;, &quot;bottle&quot;). <strong>Zero-Shot</strong> models like <strong>CLIP</strong> (Contrastive Language-Image Pre-training) or <strong>LlaVA</strong> (Large Language-and-Vision Assistant) can recognize <em>any</em> object described in text.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-workflow">The Workflow<a href="#the-workflow" class="hash-link" aria-label="Direct link to The Workflow" title="Direct link to The Workflow" translate="no">​</a></h3>
<ol>
<li class=""><strong>Input</strong>: Image + Question (&quot;Where is the red cup?&quot;).</li>
<li class=""><strong>Model</strong>: Encodes image and text into a shared embedding space.</li>
<li class=""><strong>Output</strong>: Bounding box coordinates or a text answer.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="code-example-vqa-with-transformers">Code Example: VQA with Transformers<a href="#code-example-vqa-with-transformers" class="hash-link" aria-label="Direct link to Code Example: VQA with Transformers" title="Direct link to Code Example: VQA with Transformers" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pipeline</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> PIL </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> Image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> requests</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Load Zero-Shot Object Detection pipeline</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">detector </span><span class="token operator">=</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;google/owlvit-base-patch32&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;zero-shot-object-detection&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">find_object</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> description</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    predictions </span><span class="token operator">=</span><span class="token plain"> detector</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        candidate_labels</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">description</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># predictions = [{&#x27;box&#x27;: {&#x27;xmin&#x27;: 32, &#x27;ymin&#x27;: 50, ...}, &#x27;score&#x27;: 0.99, &#x27;label&#x27;: &#x27;red cup&#x27;}]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> predictions</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-grounding-pipeline">The Grounding Pipeline<a href="#the-grounding-pipeline" class="hash-link" aria-label="Direct link to The Grounding Pipeline" title="Direct link to The Grounding Pipeline" translate="no">​</a></h2>
<p><strong>Grounding</strong> is the process of linking a linguistic symbol (&quot;red cup&quot;) to a physical entity in the world (ID: 42, Pos: [0.5, 0.2, 0.8]).</p>
<ol>
<li class=""><strong>Speech</strong>: User says &quot;Pick up the red cup.&quot;</li>
<li class=""><strong>Intent Parser</strong>: Extracts target object description: <code>target=&quot;red cup&quot;</code>.</li>
<li class=""><strong>Vision</strong>: VQA Node searches the current camera frame for &quot;red cup&quot;.</li>
<li class=""><strong>Fusion</strong>:<!-- -->
<ul>
<li class="">If object found: Convert 2D bounding box to 3D coordinates using Depth camera.</li>
<li class="">Send <code>Pick(x,y,z)</code> command to the robot arm.</li>
<li class="">Respond via TTS: &quot;Picking up the red cup.&quot;</li>
<li class="">If not found: Respond: &quot;I don&#x27;t see a red cup.&quot;</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="handling-ambiguity">Handling Ambiguity<a href="#handling-ambiguity" class="hash-link" aria-label="Direct link to Handling Ambiguity" title="Direct link to Handling Ambiguity" translate="no">​</a></h2>
<p>What if the user says &quot;Pick up the cup,&quot; but there are two cups? A robust system detects this ambiguity.</p>
<ol>
<li class=""><strong>Count</strong>: VQA detects 2 objects matching &quot;cup&quot;.</li>
<li class=""><strong>Dialogue Policy</strong>:<!-- -->
<ul>
<li class="">If count == 1: Proceed.</li>
<li class="">If count &gt; 1: Ask for clarification.</li>
<li class=""><strong>Robot</strong>: &quot;I see two cups. Do you mean the left one or the right one?&quot;</li>
</ul>
</li>
<li class=""><strong>Context</strong>: The user replies &quot;The left one.&quot; The system updates the target filter.</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>By combining LLMs for logic, VQA for perception, and TTS/STT for communication, we create a system that feels intelligent. The robot isn&#x27;t just executing code; it&#x27;s collaborating with you in the physical world.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter-23-multimodal.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/part6/chapter-22-speech-nlu"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 22: Speech &amp; NLU</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#zero-shot-visual-question-answering-vqa" class="table-of-contents__link toc-highlight">Zero-Shot Visual Question Answering (VQA)</a><ul><li><a href="#the-workflow" class="table-of-contents__link toc-highlight">The Workflow</a></li><li><a href="#code-example-vqa-with-transformers" class="table-of-contents__link toc-highlight">Code Example: VQA with Transformers</a></li></ul></li><li><a href="#the-grounding-pipeline" class="table-of-contents__link toc-highlight">The Grounding Pipeline</a></li><li><a href="#handling-ambiguity" class="table-of-contents__link toc-highlight">Handling Ambiguity</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/part1/foundations-physical-ai">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/dashboard">Dashboard</a></li><li class="footer__item"><a class="footer__link-item" href="/chatbot-test">AI Assistant</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Platform</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">About RoboLearn</a></li><li class="footer__item"><a href="https://discord.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Community<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Privacy Policy</a></li><li class="footer__item"><a class="footer__link-item" href="/">Terms of Service</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">© 2025 RoboLearn. Built for the Next Generation of Robotics.</div></div></div></footer><section class="Toastify" aria-live="polite" aria-atomic="false" aria-relevant="additions text" aria-label="Notifications Alt+T"></section><button class="floating-button" aria-label="Open chat"><svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square" aria-hidden="true"><path d="M22 17a2 2 0 0 1-2 2H6.828a2 2 0 0 0-1.414.586l-2.202 2.202A.71.71 0 0 1 2 21.286V5a2 2 0 0 1 2-2h16a2 2 0 0 1 2 2z"></path></svg></button></div>
</body>
</html>