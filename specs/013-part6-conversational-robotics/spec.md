## Clarifications

### Session 2025-12-07
- Q: LLM Provider Choice? → A: **Local LLMs (Ollama/Llama 3)** (Free, private, no internet needed).
- Q: Speech-to-Text Engine? → A: **Vosk (Offline)** (Fast, reliable, no API keys).
- Q: Vision-Language Approach? → A: **Zero-Shot VQA (e.g., CLIP/LlaVA)** (Modern approach, no training required).

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Chapter 21: Integrating GPT Models (Priority: P1)

As a reader, I want to integrate a Large Language Model (like GPT-4 or Llama) with a robot so that it can understand and generate natural language responses.

**Why this priority**: LLMs are the core engine of modern conversational AI.

**Independent Test**: Reader can send a text prompt to the robot and receive a context-aware text response generated by the LLM.

**Acceptance Scenarios**:

1. **Given** a text input "Tell me a joke", **When** the reader sends it to the robot's LLM node, **Then** the robot outputs a relevant text response.
2. **Given** a robot with a "persona" (e.g., a helpful assistant), **When** the reader asks "Who are you?", **Then** the response aligns with the defined persona.

---

### User Story 2 - Chapter 22: Speech Recognition and NLU (Priority: P2)

As a reader, I want to enable the robot to hear and understand spoken language so that I can interact with it hands-free.

**Why this priority**: Speech is the most natural modality for human-robot interaction.

**Independent Test**: Reader can speak a command and see it transcribed and classified into an intent.

**Acceptance Scenarios**:

1. **Given** a microphone input of "Turn right", **When** the reader speaks, **Then** the system transcribes the audio to text.
2. **Given** the transcribed text "Go to the kitchen", **When** the NLU module processes it, **Then** the system extracts the intent `NAVIGATE` and entity `KITCHEN`.

---

### User Story 3 - Chapter 23: Multi-Modal Interaction (Priority: P2)

As a reader, I want to combine speech, gesture, and vision so that the robot can understand context like "Pick up *that* cup".

**Why this priority**: Resolves ambiguity and enables grounded interaction.

**Independent Test**: Reader can point at an object while speaking, and the robot correctly identifies the target object.

**Acceptance Scenarios**:

1. **Given** a visual scene with two cups, **When** the reader points to the red cup and says "This one", **Then** the robot identifies the red cup as the target.
2. **Given** a conflicting signal (pointing left, saying "right"), **When** the system processes the inputs, **Then** it asks for clarification or prioritizes one modality (defined by policy).

---

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: Chapter 21 MUST demonstrate how to connect a ROS 2 node to a **Local LLM via Ollama**.
- **FR-002**: Chapter 21 MUST explain prompt engineering techniques for robots (system prompts, few-shot learning for control).
- **FR-003**: Chapter 22 MUST cover real-time Speech-to-Text (STT) integration using **Vosk**.
- **FR-004**: Chapter 22 MUST explain Text-to-Speech (TTS) synthesis for robot voice output.
- **FR-005**: Chapter 23 MUST demonstrate fusing vision (bounding boxes/segmentation) with language (referring expressions).
- **FR-006**: Chapter 23 MUST provide a tutorial on building a **Zero-Shot VQA** pipeline using models like CLIP or LlaVA.

### Key Entities

- **LLM Node**: ROS 2 node managing the context window and API calls.
- **Interaction Context**: A short-term memory buffer storing recent dialogue and visual observations.
- **Intent**: The structured goal extracted from natural language (e.g., `GRASP`, `NAVIGATE`).
- **Grounding**: The mapping of language symbols (words) to physical world objects (IDs/Coordinates).

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: The LLM integration (Ch 21) has a latency of < 2 seconds for short responses.
- **SC-002**: The Speech Recognition (Ch 22) achieves > 90% word accuracy in a quiet environment.
- **SC-003**: The Multi-modal pipeline (Ch 23) correctly identifies the target object in 4/5 test cases involving pointing + speech.
- **SC-004**: Readers can build a "Chatbot Robot" demo that answers questions about its environment.

## Assumptions

- The reader has a working internet connection for API-based LLMs (OpenAI) or sufficient RAM (16GB+) for local LLMs.
- The robot platform has a microphone and speaker (or the simulation computer does).
- We assume the use of ROS 2 as the middleware for connecting these AI modules.